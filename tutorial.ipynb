{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://upload.wikimedia.org/wikipedia/commons/0/0c/Blender_logo_no_text.svg\" alt=\"Blender Logo\" width=\"50\">  NeuroBlender: Using Blender for Neuroscience \n",
    "This notebook contains the code to import the neuron meshes, the segmentation images and the EM images into Blender. And concisely explains how to scale the images and meshes to the correct size for rendering. This is a starter guide for using Blender to make animation and renders of neurons. I hope it serves!\n",
    "\n",
    "This code utilizes **CloudVolume** and **MeshParty** to retrieve both images and meshes.  \n",
    "\n",
    "## Covered Topics: \n",
    "- Acquiring the neuron meshes\n",
    "- Acquiring the segmentation images\n",
    "- Acquiring the EM images\n",
    "- Scaling the images to the correct size\n",
    "- Coloring the neurons appropriately\n",
    "- Importing the meshes into Blender\n",
    "- Importing the images into Blender\n",
    "- Coloring the neurons appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Objective Scene](tutorial_images/objective.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have the following dependencies installed:\n",
    "\n",
    "- **Blender**: Installed (Version used: **4.2.1** on macOS)\n",
    "- **Python**: Installed (Version used: **3.10.16**)\n",
    "- **CloudVolume**: Installed (Version used: **11.2.0**)\n",
    "- **MeshParty**: Installed (Version used: **1.18.2**)\n",
    "- **Open3D**: Installed (Version used: **0.19.0**)\n",
    "- **Matplotlib**: Installed (Version used: **3.10.1**)\n",
    "\n",
    "If you need to install missing dependencies, you can use the following commands if using conda:\n",
    "\n",
    "```bash\n",
    "conda create -n blender_env python=3.10\n",
    "conda activate blender_env\n",
    "conda install -c conda-forge open3d numpy matplotlib\n",
    "pip install cloud-volume meshparty caveclient\n",
    "```\n",
    "\n",
    "If you prefer using virtualenv:\n",
    "\n",
    "```bash\n",
    "python -m venv blender_env\n",
    "source blender_env/bin/activate\n",
    "pip install numpy cloud-volume meshparty caveclient open3d matplotlib\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Imports, Run this Block\"\"\"\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from cloudvolume import CloudVolume\n",
    "from meshparty import trimesh_io\n",
    "from cloudvolume import Bbox\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition Functions  \n",
    "\n",
    "This section contains functions to retrieve neuron meshes, segmentation images, and EM images. These functions use specified sources, bounding boxes, and resolutions to fetch and save the required data.  \n",
    "\n",
    "### Functions:  \n",
    "\n",
    "- **`get_em_images(em_src, bbox, mip_resolution, save_dir)`**  \n",
    "  Retrieves EM images from the given source within the specified bounding box and resolution, then saves them to the designated directory.  \n",
    "\n",
    "- **`get_segmentation_images(seg_src, bbox, mip_resolution, save_dir)`**  \n",
    "  Acquires segmentation images from the provided source, using the defined bounding box and resolution, and saves them to the target directory.  \n",
    "\n",
    "- **`acquire_neuron_meshes(seg_ids, color_map, use_bounds, min_bounds, max_bounds, output_dir, offset, trimesh_meta)`**  \n",
    "  Fetches neuron meshes based on segmentation IDs, applies coloring, and optionally clips them to the specified bounds before saving to the output directory.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Core functions: Run this Block\"\"\"\n",
    "def get_em_images(em_src, bbox, mip_resolution:np.array=[16,16,40], save_dir:str='./objects/default_save/em_images'):\n",
    "    em_cv = CloudVolume(em_src, mip=mip_resolution, bounded=False, fill_missing=True, use_https=True, progress=True)\n",
    "    em = em_cv[bbox]\n",
    "\n",
    "    if save_dir is not None:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        em.save_images(save_dir,)\n",
    "    return em\n",
    "\n",
    "def get_segmentation_images(seg_src, bbox, mip_resolution:np.array=[16,16,40], save_dir:str='./objects/default_save/segmentation_images', seg_ids:np.array=None):\n",
    "    seg_cv = CloudVolume(seg_src, mip=mip_resolution, bounded=False, fill_missing=True, use_https=True, progress=True)\n",
    "    seg = seg_cv[bbox]\n",
    "    if seg_ids is not None:\n",
    "        # set to 0 all the values that are not in seg_ids\n",
    "        seg[np.isin(seg, seg_ids, invert=True)] = 0\n",
    "    if save_dir is not None:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        seg.save_images(save_dir)\n",
    "    return seg\n",
    "\n",
    "def acquire_neuron_meshes(\n",
    "    seg_ids:np.array, \n",
    "    color_map:dict, \n",
    "    use_bounds:bool=False, \n",
    "    min_bounds:np.array=[0,0,0], \n",
    "    max_bounds:np.array=[0,0,0], \n",
    "    output_dir:str='./objects/default_save', \n",
    "    offset:np.array=[0,0,0], \n",
    "    trimesh_meta:trimesh_io.MeshMeta=None):\n",
    "    \"\"\"\n",
    "    Acquires the neuron meshes from the segmentation images and saves them as obj files.\n",
    "    \"\"\"\n",
    "    if trimesh_meta is None:\n",
    "        raise ValueError(\"Trimesh_meta must be provided\")\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for seg_id in seg_ids:\n",
    "        try:\n",
    "            print(f\"Processing segment ID: {seg_id}\")\n",
    "            mesh = trimesh_meta.mesh(seg_id=int(seg_id))\n",
    "            \n",
    "            # Convert trimesh to Open3D format\n",
    "            vertices = np.array(mesh.vertices)\n",
    "\n",
    "            faces = np.array(mesh.faces)\n",
    "            # Downsample the mesh\n",
    "            #original_num_triangles = len(open3d_mesh.triangles)\n",
    "            #target_number_of_triangles = max(original_num_triangles // downsampling_factor, 1)\n",
    "            #simplified_mesh = open3d_mesh.simplify_quadric_decimation(target_number_of_triangles)\n",
    "            # Filter the vertices based on bounding box\n",
    "            vertex_flags = []\n",
    "            filtered_vertices = []\n",
    "            current_true_idx = 0\n",
    "\n",
    "            if use_bounds:\n",
    "                for i, (x, y, z) in enumerate(vertices):\n",
    "                    if (min_bounds[0] <= x <= max_bounds[0] and\n",
    "                        min_bounds[1] <= y <= max_bounds[1] and\n",
    "                        min_bounds[2] <= z <= max_bounds[2]):\n",
    "\n",
    "                        x -= offset[0]\n",
    "                        y -= offset[1]\n",
    "                        z -= offset[2]\n",
    "\n",
    "                        vertex_flags.append((True, current_true_idx))\n",
    "                        filtered_vertices.append([x, y, z])\n",
    "                        current_true_idx += 1\n",
    "                    else:\n",
    "                        vertex_flags.append((False, None))\n",
    "\n",
    "                filtered_faces = []\n",
    "                for face in faces:\n",
    "                    remapped_face_indices = []\n",
    "                    for idx in face:\n",
    "                        flag, new_idx = vertex_flags[idx]\n",
    "                        if flag:\n",
    "                            remapped_face_indices.append(new_idx)\n",
    "                    \n",
    "                    if len(remapped_face_indices) == 3: \n",
    "                        filtered_faces.append(remapped_face_indices)\n",
    "            else:\n",
    "                for x, y, z in vertices:\n",
    "                    x -= offset[0]\n",
    "                    y -= offset[1]\n",
    "                    z -= offset[2]\n",
    "                    filtered_vertices.append([x, y, z])\n",
    "                filtered_faces = faces\n",
    "\n",
    "            open3d_mesh = o3d.geometry.TriangleMesh()\n",
    "            open3d_mesh.vertices = o3d.utility.Vector3dVector(np.array(filtered_vertices))\n",
    "            open3d_mesh.triangles = o3d.utility.Vector3iVector(np.array(filtered_faces))\n",
    "\n",
    "            if seg_id in color_map:\n",
    "                color = color_map[seg_id]\n",
    "            else:\n",
    "                color = [1, 1, 1]  \n",
    "            colors = np.tile(color, (len(filtered_vertices), 1))\n",
    "\n",
    "            open3d_mesh.vertex_colors = o3d.utility.Vector3dVector(colors)\n",
    "            o3d.io.write_triangle_mesh(f'{output_dir}/seg_{seg_id}.obj', open3d_mesh)\n",
    "\n",
    "            mesh_filename = f'{output_dir}/seg_{seg_id}.obj'\n",
    "            material_filename = f'{output_dir}/seg_{seg_id}.mtl'\n",
    "            relative_material_filename = f'seg_{seg_id}.mtl'\n",
    "\n",
    "            with open(material_filename, 'w') as mtl_file:\n",
    "                mtl_file.write(f\"newmtl SegmentMaterial_{seg_id}\\n\")\n",
    "                mtl_file.write(f\"Kd {color[0]} {color[1]} {color[2]}\\n\")  # Diffuse color\n",
    "                mtl_file.write(f\"Ka {color[0]} {color[1]} {color[2]}\\n\")  # Ambient color\n",
    "                mtl_file.write(f\"Ks 1.0 1.0 1.0\\n\")  # Specular color\n",
    "                mtl_file.write(\"Ns 1000\\n\")  # Specular exponent\n",
    "            with open(mesh_filename, 'r') as obj_file:\n",
    "                obj_lines = obj_file.readlines()\n",
    "            with open(mesh_filename, 'w') as obj_file:\n",
    "                obj_file.write(f\"mtllib {relative_material_filename}\\n\") \n",
    "                obj_file.write(f\"usemtl SegmentMaterial_{seg_id}\\n\")  \n",
    "                obj_file.writelines(obj_lines)\n",
    "            print(f\"Processed segment ID: {seg_id} and saved to {mesh_filename} with material {material_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing segment {seg_id}: {e}\")\n",
    "    print(\"All meshes have been processed, filtered and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring neuron meshes\n",
    "\n",
    "Blender represents objects as a collection of polygons (triangles essentially). To show a neuron, we need to get the mesh in an obj file, associate a material (color) to it and then import it in Blender.\n",
    "It is important to track scales between the voxel space and the Blender space. A blender object has to be around the order of magnitude of meters to be correctly displayed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define the segmentation source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_ids = np.array([720575940617386708])\n",
    "seg_src = 'precomputed://gs://flywire_v141_m783'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the colors you want to use for the neurons. Defaults to tab20 color map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run this Block\"\"\"\n",
    "num_colors = len(segment_ids)\n",
    "print(f\"Number of unique IDs: {num_colors}\")\n",
    "cmap = plt.get_cmap('tab20', len(segment_ids))  \n",
    "color_map = {segment_id: cmap(i)[:3] for i, segment_id in enumerate(segment_ids)}  # Map IDs to colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the trimesh mesh meta object. That object will be used to acquire the meshes. We also define the colors we want to use for the neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Preparing Trimesh, Run this Block\"\"\"\n",
    "\n",
    "mm = trimesh_io.MeshMeta(\n",
    "    cv_path=seg_src,\n",
    "    disk_cache_path=\"flywire_cache\",\n",
    "    map_gs_to_https=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Origin (offset)\n",
    "\n",
    "Properly defining the neuron offset is **critical** to ensure correct positioning in Blender.  \n",
    "\n",
    "If the offset is not set, the neuron will be displayed at its voxel coordinates, which are typically on the order of **1e9**. Since Blender does not interpret voxel resolution, it will render the neuron at **1e9 meters**, causing extreme misplacement. Getting to see the neuron is feasible but will take a lot of time if you have not set the offset correctly.\n",
    "\n",
    "To prevent this, we define the offset **in voxel space** and convert it to nm, ensuring accurate positioning and scale within Blender.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = np.array([146739, 42915, 900]) # TODO: change freely. This will mean this point will be at the origin in Blender space.\n",
    "em_resolution = np.array([4,4,40]) # TODO: change accordingly to the resolution of the EM images.\n",
    "blender_origin = offset * em_resolution # to convert to nm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mesh Acquisition.\n",
    "\n",
    "They will be saved each as their own obj file, with a matching mtl file. You can define a bounding box to limit the mesh to a specific region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquire_neuron_meshes(segment_ids, color_map, use_bounds=False, output_dir='./default_save/neuron_meshes/', offset=blender_origin, trimesh_meta=mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing `.obj` Files in Blender\n",
    "\n",
    "## Steps for Importing:\n",
    "1. **Import as `.obj`**:  \n",
    "   - Go to **Files > Import > Wavefront (.obj)**.  \n",
    "   - **Select all neurons wanted**. \n",
    "\n",
    "2. **Import Axis Configuration**:  \n",
    "   - **Forward Axis: `Y`**  \n",
    "   - **Up Axis: `Z`**  \n",
    "\n",
    "3. **After Importing**:  \n",
    "   - The **neuron should be huge**.  \n",
    "   - Add an **Empty** object.  \n",
    "   - Make the neuron a **child** of the empty:\n",
    "     - Press `Shift + A`, select **Empty**.  \n",
    "     - Select both neuron and empty, then press `Cmd + P`, choose **Default Parenting**.  \n",
    "\n",
    "4. **Scaling**:  \n",
    "   - Scale the empty by **1e-4 or 1e-3**.  \n",
    "   - **Recommended: scale by`1e-4`**. Meaning `1 meter` in Blender = `10000 nm` in real life.  \n",
    "\n",
    "5. **Visibility Check**:  \n",
    "   - If the object is not visible:  \n",
    "     1. Adjust **scales** and check **Dimensions tab**.  \n",
    "     2. Verify the **viewport visualization mode**.  \n",
    "\n",
    "## **Performance Tip**:\n",
    "- To **reduce load**, scale at **import time**.  \n",
    "- Keep objects in **nm units** and use the empty for Blender scaling.  \n",
    "- You can also adjust **decimation** in `acquire_neuron_meshes()`.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example Neurons Imported](tutorial_images/example_neurons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring EM and Segmentation Images  \n",
    "\n",
    "We use [CloudVolume](https://github.com/seung-lab/cloud-volume) to obtain EM and segmentation images. First, we define the source of the images and the bounding box for the region of interest.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_src = 'precomputed://https://bossdb-open-data.s3.amazonaws.com/flywire/fafbv14'\n",
    "seg_src = 'precomputed://gs://flywire_v141_m783'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define here the image resolution you want (the quality of the image). \n",
    "\n",
    "Tip: Higher quality images will take longer to load and render in Blender. If you build a stack of images, consider only using the first image in high resolution and the rest in low resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_image_resolution = np.array([16,16,40]) # this affects the resolution of the segmentation images. Doesn't affect the size in Blender, just quality.\n",
    "em_image_resolution = np.array([16,16,40]) # this affects the resolution of the EM images. Same as above.\n",
    "\n",
    "# Lower resolution would be [32,32,40] for example. You can check with CloudVolume how many mip levels are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the box we want to have of images. In this example, we want a cube of images and we want it to be centered on the origin and the images going down the Z-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_image_ctr = offset\n",
    "em_resolution = np.array([4,4,40]) # Voxel resolution of the EM images\n",
    "img_size_x = 1000 \n",
    "img_size_y = 1000\n",
    "img_size_z = 100   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = Bbox([first_image_ctr[0] - img_size_x, first_image_ctr[1] - img_size_y, first_image_ctr[2] - 2*img_size_z+1], \n",
    "            [first_image_ctr[0] + img_size_x, first_image_ctr[1] + img_size_y, first_image_ctr[2] + 1]) \n",
    "bbox *= em_resolution\n",
    "bbox.unit = 'nm'\n",
    "size_x_nm = 2*img_size_x*em_resolution[0]\n",
    "size_y_nm = 2*img_size_y*em_resolution[1]\n",
    "size_z_nm = 2*img_size_z*em_resolution[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_cutout = get_segmentation_images(seg_src, bbox, mip_resolution=seg_image_resolution, save_dir='./default_save/segmentation_images/', seg_ids=segment_ids)\n",
    "print(f\"Size of the segmentation image in nm: {size_x_nm} x {size_y_nm} x {size_z_nm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_cutout = get_em_images(em_src, bbox, mip_resolution=em_image_resolution, save_dir='./default_save/em_images/')\n",
    "print(f\"Size of the EM image in nm: {size_x_nm} x {size_y_nm} x {size_z_nm}. Images are spaced by {em_resolution[2]} nm in the Z-axis and inverted in the Y-axis (compared to neuroglancer).\")\n",
    "print(f\"Import image mesh plane (Shift + A, then Image, then Image Mesh Plane). Set height to {size_x_nm} m and Z-offset to {em_resolution[2]} m.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When importing the images, use **<span style=\"color:red;\">offset_direction = -Z</span>** and the distance between images to be **<span style=\"color:red;\">40 m</span>** and align to the **<span style=\"color:red;\">+Z axis</span>**.  \n",
    "Also use the height corresponding to **<span style=\"color:red;\">size_x_nm</span>**.  \n",
    "\n",
    "Then add an **<span style=\"color:red;\">empty</span>** for scaling, scale it with the same ratio you used for the neurons (**<span style=\"color:red;\">1e-4</span>**).  \n",
    "**<span style=\"color:red;\">Invert the Y-axis</span>** of the empty to be exactly as in **<span style=\"color:red;\">Neuroglancer</span>**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical end result.\n",
    "\n",
    "You should end up with something like this.\n",
    "You should expect to have the section 000 to be the lowest in the Z-direction as CloudVolume saves them in the order of the Z-axis.\n",
    "I recommend using a collection for each of the objects and having a different scaling empty for the EM and the neuron meshes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![End Result](tutorial_images/end_result.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going further in cool visuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neurons as imported look jagged. You can apply smoothing to them by right clicking on the mesh and selecting \"Shade Smooth\".\n",
    "\n",
    "One cool thing to do is to melt the EM stack. To hack this, you can just create a cube (Shift + A, then Mesh, then Cube) and scale it to the size of the EM stack.\n",
    "Then make it transparent and disable it in Shadows and View (Select the cube, then in Object Properties, disable Shadow). The cube should now be invisible. In the below photo the Ray visibility parameters, disable both camera and Shadow.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cube Properties](tutorial_images/cube_in_blender.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then add a Boolean modifier to all the images in respect to the cube. (Select the image, then in the Modifier tab, add a Boolean modifier, then select the cube as the target, and set the operation to Difference.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Boolean Modifier](tutorial_images/modifier.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a collection, you can also add a Boolean modifier to the collection in respect to the cube, instead of applying it to each image which get tedious.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go in Scripting tab (top middle-ish), copy-paste and run the following script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bpy\n",
    "\n",
    "cube = bpy.data.objects.get(\"Cube\") # TODO:CHANGE THIS TO THE NAME OF YOUR INTERSECTING OBJECT\n",
    "\n",
    "# Get the \"EM_images\" collection\n",
    "collection = bpy.data.collections.get(\"EM_images\") # TODO: CHANGE THIS TO THE NAME OF YOUR COLLECTION\n",
    "\n",
    "if cube and collection:\n",
    "    for obj in collection.objects:  \n",
    "        if obj.type == 'MESH':\n",
    "            # Check if a Boolean modifier already exists, otherwise add one\n",
    "            boolean_mod = None\n",
    "            for mod in obj.modifiers:\n",
    "                if mod.type == 'BOOLEAN':\n",
    "                    boolean_mod = mod\n",
    "                    break\n",
    "            \n",
    "            if not boolean_mod:\n",
    "                boolean_mod = obj.modifiers.new(name=\"Boolean_Cut\", type='BOOLEAN')\n",
    "\n",
    "            boolean_mod.object = cube \n",
    "            boolean_mod.operation = 'DIFFERENCE' \n",
    "            print(f\"Boolean modifier applied to {obj.name}\")\n",
    "else:\n",
    "    print(\"Error: Cube or EM_images collection not found! Check object names.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last but not least: Keyframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now we have cool objects, we didn't even have to build them, just import them.\n",
    "But they are static... That does not make a great movie.\n",
    "\n",
    "The core of video animation in Blender is setting starting points or ending points for a transition of an object: using keyframes.\n",
    "\n",
    "To make an object move, you set a keyframe on the first frame where the object should start the movement, setting the keyframe for the object's position, and then another keyframe on the last frame where the object should end the movement, setting the keyframe for the object's position again. There is a full tutorial on the Seung Lab slack, ask me for access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <video controls width=\"1200\">\n",
    "  <source src=\"tutorial_images/example_keyframes.mov\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video> -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the the most basic way of rendering neurons and EM data. Rendering more neurons, with larger sections, is a matter of diminshing memory usage, the complexity of each frame rendering.\n",
    "I will add the code to allow cluster rendering of a project in a future update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for reading! Contact me at levisseraphael0@gmail.com for any questions or suggestions. Thanks to the Seung Lab for the data, Sven Dorkenwald and the CaveClient team for meshparty, William Silversmith for his efficient cloudvolume library. And of course, Pr. Sebastian Seung for motivating this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blender_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
